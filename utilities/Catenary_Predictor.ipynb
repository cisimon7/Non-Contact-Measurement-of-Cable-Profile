{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Catenary_Predictor.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNr3+t41GUHkf+f0JPG40Sr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cisimon7/Non-Contact-Measurement-of-Cable-Profile/blob/main/utilities/Catenary_Predictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWdd4qwZXUHS"
      },
      "source": [
        "### Finding the Equation of the Cable Profile\r\n",
        "\r\n",
        "The equation of the cable profile can be defined as:<br><br>\r\n",
        "$y = a \\: cosh(\\frac{x+\\alpha}{a}) + \\beta$ <br><br>\r\n",
        "\r\n",
        "$y' = \\frac{dy}{dx} = sinh(\\frac{x+\\alpha}{a})$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7RDqn-OUQ1s"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhhvfOJbVGWD"
      },
      "source": [
        "# Defining Function using torch api \r\n",
        "def lsq_catenary(x, a, b, c):\r\n",
        "  return b + a * (torch.cosh((x - c) / a) - 1)  # Why the -1 ??"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qw9C68XmWgm7"
      },
      "source": [
        "# Calculate the loss\r\n",
        "def lsq_loss(y, y_hat, error=lambda x: torch.norm(x)):\r\n",
        "    return error(y - y_hat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hyM9MJZWkEp"
      },
      "source": [
        "# The predictor class\r\n",
        "# Given the points of x and y tries to predict the catenary that will fit to all of them\r\n",
        "# Uses pytorch graidents\r\n",
        "class CatenaryPredictor:\r\n",
        "    def __init__(self, x, y, error=lambda x: torch.norm(x)):\r\n",
        "        self.x = x\r\n",
        "        self.y = y\r\n",
        "        self.error_func = error\r\n",
        "        \r\n",
        "        # Init a,b,c arguments\r\n",
        "        self.a = torch.tensor(0.5, dtype=torch.float32, requires_grad=True)\r\n",
        "        self.b = torch.tensor(0.5, dtype=torch.float32, requires_grad=True)\r\n",
        "        self.c = torch.tensor(0.5, dtype=torch.float32, requires_grad=True)\r\n",
        "\r\n",
        "        # Init optimizer and scheduler\r\n",
        "        self.optimizer = torch.optim.SGD([self.a, self.b, self.c], lr=1e-3)\r\n",
        "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, 'min', patience=1000, factor=0.80)\r\n",
        "\r\n",
        "    # Fit the catenary\r\n",
        "    def solve(self, iters=10000, verbosity=1000):\r\n",
        "        # Used for early stopping\r\n",
        "        prev_loss = 0  # Save previous loss\r\n",
        "        patience = 0  # how many times losses have been the same\r\n",
        "\r\n",
        "        for i in range(iters):\r\n",
        "            self.optimizer.zero_grad()\r\n",
        "\r\n",
        "            # Calculate new ys\r\n",
        "            ys = lsq_catenary(self.x, self.a, self.b, self.c)\r\n",
        "\r\n",
        "            # Calculate how different the ys are\r\n",
        "            loss = lsq_loss(self.y, ys, self.error_func)\r\n",
        "\r\n",
        "            if i % verbosity == 0:\r\n",
        "                print(f'Epoch:{i}.  Loss:{loss}')\r\n",
        "\r\n",
        "            if loss == prev_loss:  # If losses were the same, increase counter\r\n",
        "                patience += 1\r\n",
        "            else:\r\n",
        "                patience = 0  # Otherwise, reset\r\n",
        "\r\n",
        "            if patience == 1000:  # If reached 1000 same epochs \r\n",
        "                break\r\n",
        "\r\n",
        "            prev_loss = loss\r\n",
        "            loss.backward()  # Update using gradient\r\n",
        "            self.optimizer.step()\r\n",
        "            self.scheduler.step(loss)\r\n",
        "\r\n",
        "        return self.a, self.b, self.c, ys"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}